# Cooperative Cruising: Reinforcement Learning based Time-Headway Control for Increased Traffic Efficiency

This repository contains the source code to reproduce the experiments in our paper "Cooperative Cruising: Reinforcement Learning based Time-Headway Control for Increased Traffic Efficiency."

If you find this repository helpful in your publications, please consider citing our paper.

## Introduction
The proliferation of automated vehicles equipped with advanced driver assistance systems (ADAS) represents an opportunity for improving driving efficiency and alleviating traffic congestion.
However, proposed ADAS-based congestion reduction methods often assume connectivity, perception, and control capabilities that are not widely available in vehicles.

We propose a dynamic time-headway control approach for increasing the average travel speed of vehicles while maintaining safety in high-volume highway traffic, utilizing existing traffic estimation technology and low-bandwidth vehicle-to-infrastructure connectivity.

At the core of our approach is a safe, reinforcement learning-based controller that dynamically communicates desired time headways to automated vehicles based on real-time traffic conditions as they approach bottlenecks. These desired time headways are then used by automated driving systems, such as Adaptive Cruise Control (ACC), to adjust their following distance. Our approach outperforms both simulated human-driven traffic and an optimized fixed headway control scheme in highway simulation featuring hundreds of vehicles with complex lane-change and car-following interactions, across a variety of traffic conditions.

Our control system is designed to reduce the gap to real-world deployment by (i) leveraging available connectivity, traffic estimation, and ADAS technology, and (ii) maintaining vehicle safety by building on top of existing ACC systems. Our system thus offers a potentially scalable and practical approach that could positively impact numerous road users.

## Dependencies

To configure a python environment to run our code:
1. Install SUMO 1.17.
1. Clone the code in this repository.
1. Edit the environment.yml file:
    - Replace PATH\TO\Eclipse\Sumo\tools with the actual path to the tools directory under the SUMO installation directory.
    - Replace PATH\TO\CODE\DIR with the actual path to the cloned code directory.
1. install the conda environment using the edited environment.yml file.

List of main libraries used:
+ Python 3.x/numpy/scipy/pandas/matplotlib
+ Ray [RLlib](https://docs.ray.io/en/latest/rllib/index.html) 2.7: *A library with RL algorithm implementations and parallel training*
+ [PyTorch](https://pytorch.org) 2.1
+ Farma Foundation [gymnasium](https://gymnasium.farama.org/) 0.28: *An API standard for reinforcement learning environments*
+ [tqdm](https://tqdm.github.io/): *A library for smart progress bars*

## Instructions

### Running experiments

Experiments can be run the following command. To reproduce the results in our paper, use default parameters, and append the flag `--random_seed 0` to the command below.

```
python train_ppo.py
```
Optional parameters can be specified using the following flags:
+ ACC-equipped vehicle percentage: `--av_percent <AV_PERCENT>`. Default: 100
+ Use single-lane scenario: `--single_lane`. Default: False
+ Number of segments before the bottleneck within which to control ACC-equipped vehicles: `--num_control_seg <NUM_CONTROL_SEGMENTS>`. Default: 2
+ Simulation time horizon: `--sim_time <SIMULATION_TIME_HORIZON>`. Default: 500
+ Number of parallel rollout workers: `--num_workers <NUM_WORKERS>`. Default: 10

### Evaluating controllers

After training using to code above, RL controllers can be evaluated using the following command. If optional parameters were specified in the training command, make sure to include them in the evaluation command as well.
```
python evaluate_control.py <CHECKPOINT_DIR_PATH>
```
optional parameters:
+ ACC-equipped vehicle percentage: `--av_percent <AV_PERCENT>`. Default: 100
+ Use single-lane scenario: `--single_lane`. Default: False
+ Number of segments before the bottleneck within which to control ACC-equipped vehicles: `--num_control_seg <NUM_CONTROL_SEGMENTS>`. Default: 2
+ Simulation time horizon: `--sim_time <SIMULATION_TIME_HORIZON>`. Default: 500
+ Number of tests to run: `--num_tests <NUMBER_OF_TESTS>`. Default: 30
+ Results output directory name: `--results_dir <DIRECTORY_NAME>`. Default: 'test'

### Generating plots

After running the evaluation above, plots can then be generated by running the following command. Note that:
1. The value of `--results_dir` must be the same as when running the evaluation command.
1. If you evaluated controllers for different values of av_percent, you can plot all results on a single plot by listing multiple values after the `--av_percent` option, as done in Figure 3 in the paper.
1. You must specify the same num_tests as used for the evaluation.
1. You must specify if you evaluated a single lane scenario.

```
python simulation_analysis.py
```
optional parameters:
+ ACC-equipped vehicle percentage: `--av_percent <AV_PERCENT_1> <AV_PERCENT_2> <AV_PERCENT_3> ...`. Default: 100
+ Use single-lane scenario: `--single_lane`. Default: False
+ Number of tests to run: `--num_tests <NUMBER_OF_TESTS>`. Default: 30
+ Evaluation results directory name: `--results_dir <DIRECTORY_NAME>`. Default: 'test'